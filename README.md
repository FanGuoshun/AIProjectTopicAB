# 类脑智能大作业TopicB代码实现
## 实验环境

- Windows 10
- Python 3.7.5
- Pytorch 1.3.1
- PyCharm 

## 实验思路

1.Topic A

使用相同的神经网络，不同规模的训练集进行实验，观察样本量对损失函数的影响。


2.Topic B

设计不同深度的神经网络，解决不同数据点的分类问题，观察它们对损失函数平滑性的影响程度。

使用数组记录不同深度神经网络中损失函数变化的值，绘制成曲线，观察它们的平滑程度。



## 实验过程
1.Topic A

样本中有两个子集，一个为均值为[-1,-1]标准差为1的正态分布，另一个为均值[1,1]标准差为1的正态分布。其中前者标签为0，后者为1。

样本规模分别使用10，100，1000，神经网络结构为3层隐藏层6个节点，损失函数使用交叉熵函数，激活函数使用Relu函数。

分别绘出损失函数的变化过程，观察样本量对损失函数的平滑性的影响。

2.Topic B

分别搭建了3，4，5层神经网络，并分别使用它们解决相同数据的分类问题，将它们的损失函数的变化值绘制成了散点图。
观察它们的平滑程度。

## 实验结论

1.Topic A

样本量越大，损失函数越平滑。


2.Topic B

3层神经网络的损失函数平滑性最差，4层神经网络的损失函数平滑性较好，5层的最好。

神经网络越深，损失函数的平滑性越好，但是分类的精确度却有所下降，不能盲目的增加神经网络的深度。